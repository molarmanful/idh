{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:large;\">\n",
    "<div style=\"text-align: right\">IM-UH 1511 <b>Introduction to Digital Humanities</b><br/>\n",
    "Student name: <span style=\"color:blue\"><b>Benjamin Pang</b></span><br/>\n",
    "Grade: <span style=\"color:red\"><b></b></span><br/><br/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">HOMEWORK 2</span>\n",
    "# <span style=\"color:green\">Network of Co-Occurring Words (Word-Net) in Sentences of the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3179,
     "status": "error",
     "timestamp": 1571910651398,
     "user": {
      "displayName": "Nas Gopee",
      "photoUrl": "",
      "userId": "08967367925255446352"
     },
     "user_tz": -240
    },
    "id": "IK7KzXvzKgiS",
    "outputId": "700c2d83-4889-4008-f801-56f48fe03430"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygraphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-733603b93c50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpygraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx_agraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz_layout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygraphviz'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.perf_counter()\n",
    "import urllib, os, codecs, random, operator, re, string, copy, dateutil.parser, itertools, pickle, datetime, math, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from string import punctuation, digits\n",
    "import pathlib\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "# import pygraphviz\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import networkx as nx\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib as mpl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cF7BywMOKgiZ"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your working directory\n",
    "home = str(pathlib.Path.cwd())\n",
    "\n",
    "# create a path to which the file will be written\n",
    "text_path = os.path.join(home, 'WizardofOz.txt')\n",
    "\n",
    "# location of the project gutenberg copy of the moby-dick text file\n",
    "text_url = 'http://www.gutenberg.org/cache/epub/55/pg55.txt'\n",
    "\n",
    "urllib.request.urlretrieve(text_url, text_path)\n",
    "\n",
    "print('Downloaded to:', text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = codecs.open(text_path, \"r\", encoding=\"utf-8\").readlines()\n",
    "for line in f:\n",
    "    if line.startswith(\"1.  The Cyclone\"):\n",
    "        print(f.index(line)) #198\n",
    "    if line.startswith(\"And oh, Aunt Em!  I'm so glad to be at home again!\"):\n",
    "        print(f.index(line)) #15514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ff=f[109:4756]\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ff[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text=\"\\r\\n\".join(ff)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlename = \"L. Frank Baum's The Wonderful Wizard of Oz\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "nuw=len(words)\n",
    "uw=len(set(words))\n",
    "print(\"%s contains %i nonunique and %i unique words\"%(titlename,nuw,uw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of Proper Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "d_tags = {}\n",
    "\n",
    "docs_d={\"WizardofOz\":text}\n",
    "for key, value in docs_d.items():\n",
    "    arr = []\n",
    "    doc = nlp(value.replace('\\n',''))\n",
    "    #Keep these types of nlp entities\n",
    "    keep_l = ['PERSON'] #,'NORP','PRODUCT','ORG']\n",
    "    #Typo/model error + german corrections\n",
    "    drop_t = []\n",
    "    \n",
    "    #Things inflect library handles poorly or to exclude from touching\n",
    "    ex_ls = []\n",
    "    \n",
    "    for X in doc.ents:\n",
    "        s1 = X.text\n",
    "        if (X.label_ in keep_l) and (s1.lower() not in drop_t) and (s1):\n",
    "            arr.append((s1, X.label_))\n",
    "    d_tags[key] = arr\n",
    "# pprint(d_tags)\n",
    "names=[]\n",
    "for k,v in d_tags.items():\n",
    "    for vv in v:\n",
    "        if vv[0] not in names:\n",
    "            p=vv[0].replace(\"'\",\"\")\n",
    "            p=p.title()\n",
    "            names.append(p)\n",
    "names=sorted(set(names))\n",
    "print(len(names))\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rem=[]\n",
    "for p in names:\n",
    "    if \"_\" in p:\n",
    "        rem.append(p)\n",
    "    if \"--\" in p:\n",
    "        rem.append(p)\n",
    "    if p not in text:\n",
    "        rem.append(p)\n",
    "names=[p for p in names if p not in rem]\n",
    "pp=[q for q in itertools.product(names,names) if q[0]!=q[1]]\n",
    "for q in pp:\n",
    "    if q[0] in q[1]:\n",
    "        rem.append(q[0])\n",
    "    if q[1] in q[0]:\n",
    "        rem.append(q[1])\n",
    "    w=q[0]+\" \"+q[1]\n",
    "    if w in text:\n",
    "        names.append(w)\n",
    "        rem.append(q[0])\n",
    "        rem.append(q[1])\n",
    "names=[p for p in names if p not in rem]\n",
    "names=sorted(set(names))\n",
    "print(len(names))\n",
    "sorted(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rem=['Wizard Oz', 'Gates', 'Hammer', 'Head Dorothy', 'Kalidahs', 'Lady', 'Meek', 'Munchkin', 'Sorrowfully Dorothy', \n",
    "    'tin man', 'Joker', 'This Golden Cap', 'The Wicked Witch','The Cowardly Lion', 'Witch Glinda', 'The Tin Woodman',\n",
    "    'The King Crow', 'Stork', 'Good Witch of the North', 'Quadlings']\n",
    "names=[p for p in names if p not in rem]\n",
    "names=names+['Tin Woodman', 'Scarecrow', 'Oz', 'Guardian of the Gates', 'Mr. Joker','Wicked Witch of the East',\n",
    "             'Wicked Witch of the West', 'Glinda', 'Dorothy', 'Lion', 'King Crow', 'King of the Winged Monkeys',\n",
    "             'Witch of the North', 'Hammer-Heads', 'Quadlings', 'Winkies', 'Stork', 'Kalidahs', 'Great Wizard', 'Wizard',\n",
    "            'green girl', 'Monkey King', 'Munchkins', 'Cowardly Lion', 'Wildcat', 'tin man','Wizard']\n",
    "    \n",
    "        \n",
    "names=sorted(set(names)) \n",
    "print(len(names))\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nfreq=[]\n",
    "for i in names: \n",
    "    nfreq.append(text.count(i))\n",
    "pnf_df = pd.DataFrame(\n",
    "    {'Proper Nouns': names, \n",
    "     'Frequency of Occurrences': nfreq\n",
    "    })\n",
    "pnf_df=pnf_df[['Proper Nouns','Frequency of Occurrences']]\n",
    "pnf_df=pnf_df.sort_values(by ='Frequency of Occurrences',ascending=False)\n",
    "# trf_df=trf_df[trf_df[\"Frequency of Occurrences\"]>10]\n",
    "print(len(pnf_df))\n",
    "pnf_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnf_df.to_csv('Names_freqs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pnf_df.set_index('Proper Nouns').T.to_dict() \n",
    "x=sorted([(k,v['Frequency of Occurrences']) for k,v in x.items()], key=lambda x: x[1],reverse=True)\n",
    "keys = [i for (i,j) in x] \n",
    "y_pos = np.arange(len(keys))\n",
    "performance = [j for (i,j) in x] \n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.axes()\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.6) \n",
    "ax.invert_yaxis()\n",
    "plt.yticks(y_pos, keys)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Proper Nouns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[]\n",
    "for (i,j) in x:\n",
    "    for k in range(j):\n",
    "#         print i.replace(\" \",\"_\").replace(\"-\",\"_\")\n",
    "        t.append(i.replace(\" \",\"_\").replace(\"-\",\"_\"))\n",
    "ttd=' '.join(t)\n",
    "wordcloud = WordCloud(collocations=False,background_color=\"white\",colormap=\"plasma\",width=4000,height=2000).generate(ttd)\n",
    "fig = plt.figure(figsize=(13,13))\n",
    "default_colors = wordcloud.to_array()\n",
    "plt.imshow(default_colors, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "ss=\"Proper Nouns of %s\" %titlename\n",
    "plt.suptitle(ss,fontsize=25)\n",
    "plt.tight_layout(rect=[0, 0, 1, 1.4]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of Aliased Proper Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_dict={}\n",
    "for n in names:\n",
    "    if n==\"Tin Woodman\":\n",
    "        alias_dict[n]=\"tin man\"\n",
    "    elif n==\"tin man\":\n",
    "        alias_dict[n]=\"tin man\"\n",
    "    elif n==\"Oz\":\n",
    "        alias_dict[n]=\"Oz\"\n",
    "    elif n==\"Wizard\":\n",
    "        alias_dict[n]=\"Oz\"\n",
    "    elif n==\"Great Wizard\":\n",
    "        alias_dict[n]=\"Oz\"\n",
    "    elif n==\"Cowardly Lion\":\n",
    "        alias_dict[n]=\"Lion\"\n",
    "    elif n==\"Lion\":\n",
    "        alias_dict[n]=\"Lion\"\n",
    "    elif n==\"King of the Winged Monkeys\":\n",
    "        alias_dict[n]=\"Monkey King\"\n",
    "    elif n==\"Monkey King\":\n",
    "        alias_dict[n]=\"Monkey King\"\n",
    "    else:\n",
    "        alias_dict[n]=n\n",
    "print(\"The dictionary of aliases has %i keys (names) and %i unique values (aliased proper nouns)\" %(len(alias_dict.keys()),len(set(alias_dict.values()))))\n",
    "# for k,v in alias_dict.items():\n",
    "#     print(k,\"-->\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Network of Sententially Co-Occurring Proper Names (\"Word-Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(text)\n",
    "textSentences = blob.sentences\n",
    "sendic=dict()\n",
    "for i,v in enumerate(textSentences):\n",
    "    sent=v.sentiment.polarity\n",
    "    wl=[]\n",
    "    for term in list(set(alias_dict.values())):\n",
    "        if term in v: \n",
    "            wl.append(term)\n",
    "    if len(wl)>1:\n",
    "        sendic[i]=wl \n",
    "medges=[]\n",
    "for k,v in sendic.items():\n",
    "    sent=textSentences[k].sentiment.polarity\n",
    "    dd={}\n",
    "    ps=set()\n",
    "    for j in itertools.combinations(v, 2):\n",
    "        ps.add(j)\n",
    "        dd[j]=(k,sent)\n",
    "    for jj in ps:\n",
    "        s=0\n",
    "        ss=0\n",
    "        for kk,vv in dd.items():\n",
    "            if kk==jj:\n",
    "                s+=1\n",
    "                ss+=vv[1]\n",
    "        if alias_dict[jj[0]]!=alias_dict[jj[1]]:\n",
    "            medges.append((alias_dict[jj[0]],alias_dict[jj[1]],\"Sentence_\"+str(k),ss/float(s)))\n",
    "print(\"%s contains %i sentential co-occurrences among %i aliased proper nouns\"%(titlename,len(medges),len(set(alias_dict.values()))))\n",
    "medges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medgesd=[]\n",
    "for e in medges:\n",
    "    d={}\n",
    "    d['Sentence']=e[2]\n",
    "    d['Average sentiment']=e[3]\n",
    "    medgesd.append((e[0],e[1],d))\n",
    "    \n",
    "G = nx.MultiGraph()\n",
    "G.add_edges_from(medgesd)\n",
    "for e in G.edges(data=True):\n",
    "    if e[0]==e[1]:\n",
    "        G.remove_edge(e[0],e[1])\n",
    "weight={(x,y):v for (x, y), v in Counter(G.edges()).items()}\n",
    "w_edges=[(x,y,z) for (x,y),z in weight.items()]\n",
    "Gw = nx.Graph()\n",
    "Gw.add_weighted_edges_from(w_edges)\n",
    "\n",
    "print(\"The graph of sententially co-occurrent proper nouns in %s is a weighted graph and it has %i nodes and %i edges \\n\" %(titlename,len(Gw.nodes()),len(Gw.edges())))\n",
    "out=' '.join([n+\"\\n\" for n in alias_dict.values() if n not in Gw.nodes()])\n",
    "print(\"The proper names which do not co-occur in sentences are: \\n %s\" %out)\n",
    "# print \"Graph Gw is a weighted graph with %i nodes and %i edges\" %(len(Gw.nodes()),len(Gw.edges()))\n",
    "print(\"The density of this graph is %.3f\" %nx.density(Gw))\n",
    "if nx.is_connected(Gw)==True:\n",
    "    print (\"This graph is a connected graph\")\n",
    "else:\n",
    "    print (\"This graph is a disconnected graph and it has\",nx.number_connected_components(Gw),\"connected components\" )   \n",
    "giant = max(nx.connected_component_subgraphs(Gw), key=len)\n",
    "Gwlcc=Gw.subgraph(giant)\n",
    "print (\"The largest connected component of this graph is a weighted graph with %i nodes and %i edges\" %(len(Gwlcc.nodes()),len(Gwlcc.edges())))\n",
    "print (\"The density of the largest connected component of this graph is %.3f\" %nx.density(Gwlcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_width=[Gw[u][v]['weight'] for u,v in Gw.edges()]\n",
    "edge_width=[math.log(1+w) for w in edge_width]\n",
    "cmap=plt.cm.cool\n",
    "weight_list = [ e[2]['weight'] for e in Gw.edges(data=True) ]\n",
    "edge_color=weight_list\n",
    "vmin = min(edge_color) \n",
    "vmax = max(edge_color) \n",
    "# width_list=[2*math.log(2+w) for w in weight_list]\n",
    "width_list=[1.5*math.log(abs(min(weight_list))+2+w) for w in weight_list] #weight_list\n",
    "nsi=[15*Gw.degree(n) for n in Gw.nodes()]\n",
    "\n",
    "figsize=(15,15)\n",
    "pos=graphviz_layout(Gw) \n",
    "node_color=\"#ffb3b3\"\n",
    "node_border_color=\"r\"\n",
    "plt.figure(figsize=figsize);\n",
    "nodes = nx.draw_networkx_nodes(Gw, pos, node_color=node_color,node_size=nsi)\n",
    "nodes.set_edgecolor(node_border_color)\n",
    "nx.draw_networkx_edges(Gw, pos, edge_color=edge_color,edge_cmap=cmap,vmin=vmin, vmax=vmax,width=edge_width,alpha=0.7)\n",
    "plt.axis('off');\n",
    "yoffset = {}\n",
    "y_off = -5 # offset on the y axis\n",
    "for k, v in pos.items():\n",
    "    yoffset[k] = (v[0], v[1]+y_off)\n",
    "nx.draw_networkx_labels(Gw, yoffset,font_size=10);\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, orientation='horizontal', shrink=0.7, pad = 0.02)\n",
    "cbar.set_label('Average sentiment of sentences')\n",
    "sst=\"The graph of co-occurrent proper nouns in %s \\n weighted over their average sentiment score\" %titlename\n",
    "plt.title(sst,fontsize=15);\n",
    "plt.margins(x=0.1, y=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gw=Gwlcc\n",
    "\n",
    "edge_width=[Gw[u][v]['weight'] for u,v in Gw.edges()]\n",
    "edge_width=[math.log(1+w) for w in edge_width]\n",
    "cmap=plt.cm.cool\n",
    "weight_list = [ e[2]['weight'] for e in Gw.edges(data=True) ]\n",
    "edge_color=weight_list\n",
    "vmin = min(edge_color) \n",
    "vmax = max(edge_color) \n",
    "# width_list=[2*math.log(2+w) for w in weight_list]\n",
    "width_list=[1.5*math.log(abs(min(weight_list))+2+w) for w in weight_list] #weight_list\n",
    "nsi=[15*Gw.degree(n) for n in Gw.nodes()]\n",
    "\n",
    "figsize=(15,15)\n",
    "pos=graphviz_layout(Gw) \n",
    "node_color=\"#ffb3b3\"\n",
    "node_border_color=\"r\"\n",
    "plt.figure(figsize=figsize);\n",
    "nodes = nx.draw_networkx_nodes(Gw, pos, node_color=node_color,node_size=nsi)\n",
    "nodes.set_edgecolor(node_border_color)\n",
    "nx.draw_networkx_edges(Gw, pos, edge_color=edge_color,edge_cmap=cmap,vmin=vmin, vmax=vmax,width=edge_width,alpha=0.7)\n",
    "plt.axis('off');\n",
    "yoffset = {}\n",
    "y_off = -5 # offset on the y axis\n",
    "for k, v in pos.items():\n",
    "    yoffset[k] = (v[0], v[1]+y_off)\n",
    "nx.draw_networkx_labels(Gw, yoffset,font_size=10);\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, orientation='horizontal', shrink=0.7, pad = 0.02)\n",
    "cbar.set_label('Average sentiment of sentences')\n",
    "sst=\"The largest connected component of \\n the graph of co-occurrent proper nouns in %s \\n weighted over their average sentiment score\" %titlename\n",
    "plt.title(sst,fontsize=15);\n",
    "plt.margins(x=0.1, y=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run in %.2f seconds (%.2f minutes)\" %(time.clock() - start_time,(time.clock() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2_KeywordGraphCommunitiesClassification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
